{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "242f1094-c18a-43e3-a7ec-b2e193d65c04",
   "metadata": {},
   "source": [
    "# Elementary Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e92c6-d2d2-41ed-b569-699017cf4278",
   "metadata": {},
   "source": [
    "## Fairness - Bias and Independence\n",
    "\n",
    "Suppose we toss a coin 10 times and it comes up heads every time. What should we bet for the next toss?\n",
    "\n",
    "The right answer depends on assumptions we make about **bias** and **independence** of the coin tosses.\n",
    "\n",
    "Bias here implies a tendency to favor one outcome over another. For an **unbiased** coin, a toss should provide an equal probability of either heads or tails.\n",
    "\n",
    "But suppose we have a coin that is completely unbiased - it comes as HTHTHTHT... heads and tails as alternating outcomes on each trial. This is an unbiased coin, but each toss determines the next one - thus the outcome of one trial is dependent on the previous one. What we want is some type of **randomness**, a **lack of regularity**. We want each trial to have no **memory** of the previous trials. Putting randomness another way, we want no gambling system designed to beat the system, to work - the random sequences are so unpredictable, that in the long run all gambling systems fail.\n",
    "\n",
    "We refer to these properties as **independence** - trials are independent if outcomes of trials are not influenced in any way by outcomes of previous trials.\n",
    "\n",
    "An experiment is said to be **fair** only if :\n",
    "\n",
    "- it is unbiased\n",
    "- outcomes are independent of each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2547cf2-f8fb-4c28-9a3d-c336219b1c14",
   "metadata": {},
   "source": [
    "**Urn example**\n",
    "\n",
    "Take an urn filled with colored balls. A trial consists of shaking the urn well, and noting the color. At the end of the trial if you put the ball back, that is *sampling with replacement* vs *sampling without replacement*. In the second case, after the urn is empty we replace all the balls and start again.\n",
    "\n",
    "*An unbiased, independent trial*: Let the urn be filled with equal number of red and green balls, using sampling with replacement.  \n",
    "\n",
    "*An unbiased, non-independent trial*: Urn filled with equal number of red + green balls, and repeated trials with sampling without replacement. Note that the probability will be diffeerent on each draw, but overall the setup is unbiased. However, the trials are not independent.  \n",
    "\n",
    "*A biased, independent trial*: Urn filled with 90% red + 10% green balls, and repeated trials with sampling with replacement. Overall the setup is biased. However, the trials are independent.  \n",
    "\n",
    "*A biased, non-independent trial*: Urn filled with 90% red + 10% green balls, and repeated trials with sampling without replacement. Overall the setup is biased. However, the trials are not independent.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949473a4-4448-4545-9d6d-2bc9639e37f0",
   "metadata": {},
   "source": [
    "**Gambler's fallacy**\n",
    "\n",
    "A gambler sees black turn up 12 times in a row at roulette. He argues :\n",
    "\n",
    "> This is a fair roulette wheel.  \n",
    "> Black has turned up 12 times in a row.  \n",
    "> Since the wheel is fair, red and black turn up equally often.  \n",
    "> Hence, lot of reds are due to come soon, and I should bet on red, even if it is not the next spin.  \n",
    "\n",
    "The gambler's fallacy, specifically, is to forget that a fair roulette wheel has no memory and the spins are independent. More generally, it is to assume a gambling system can be successful against a truly fair system.\n",
    "\n",
    "However, imagine we drop the first premise - we don't know that the roulette wheel is fair. Imagine a *Skeptical Gambler* arguing :\n",
    "\n",
    "> Black has come up 12 times in a row.  \n",
    "> There is a good chance the roulette wheel is biased towards black.  \n",
    "> I will start betting on black.  \n",
    "\n",
    "The *Skeptical Gambler* may be right, but we now need empirical evidence to show the wheel is biased. This reasoning is risky.\n",
    "\n",
    "And we can also argue :\n",
    "\n",
    "> If the wheel was biased for black, people would quickly catch on  \n",
    "> The casino could soon lose a lot of money  \n",
    "> Hence, it probable the wheel is unbiased\n",
    "\n",
    "This reasoning is also risky, based on our prior knowledge of casinos and wheels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5474dd6a-759c-4010-8025-07202dcf90b4",
   "metadata": {},
   "source": [
    "**Inverse Gambler's Fallacy**\n",
    "\n",
    "The gambler's fallacy can work in reverse.\n",
    "\n",
    "> Y sees X roll 4 dice and gets 4 sixes  \n",
    "> Y says X must have tried this several times\n",
    "\n",
    "Y is making the inverse gambler's fallacy. The trials are independent and trying n times should not increase your chance on the n+1th time.\n",
    "\n",
    "On the other hand :\n",
    "\n",
    "> Y hears X rolled 4 dice and got 4 sixes yesterday  \n",
    "> Y says it is likely X must have rolled the dice many times  \n",
    "\n",
    "This is not a fallacy. It is correct to think that your chances oof getting 4 sixes is higher if you try a 100 times than if you try once. The difference is that here we are not talking of a specific throw of the dice, but a time period when potentially many throws could have occured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb75cc-a899-4b02-8fa3-025b3ad80113",
   "metadata": {},
   "source": [
    "## Probability - Propositions vs Events\n",
    "\n",
    "We can talk of the probability of a proposition being true. Example :\n",
    "\n",
    "> Let proposition P = \"It will rain this Sunday\"  \n",
    "\n",
    "What is the probability P is true?  \n",
    "\n",
    "Alternatively :\n",
    "\n",
    "> Let event E = \"Rain falling on Sunday\"\n",
    "\n",
    "What is the probability event E will occur?\n",
    "\n",
    "Propositions are true or false.\n",
    "Events either occur or do not occur.\n",
    "\n",
    "Usually we can translate between these views. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20068f1-d131-4881-9f0f-55b5036fc0b5",
   "metadata": {},
   "source": [
    "## Set Operations vs Logical Connectives\n",
    "\n",
    "We can convert between events (modeled as subsets of the sample space) and propositions as follows:\n",
    "\n",
    "$A \\cup B \\equiv A \\lor B$ : Union vs disjunction\n",
    "\n",
    "$A \\cap B \\equiv A \\land B$ : Intersection vs conjunction\n",
    "\n",
    "$A^c \\equiv \\neg A$ : Complement vs negation\n",
    "\n",
    "If Pr(A) represents the probability that event A occurs, we can say :\n",
    "\n",
    "$Pr(A) = 0 \\equiv \\text{ A is false }$   \n",
    "$Pr(A) = 1 \\equiv \\text{ A is certainly true }$  \n",
    "$Pr(A) = x \\equiv \\text{ A is probably true with probability x }$   \n",
    "\n",
    "Events that are certainly true are referred to as $\\Omega$: $Pr(\\Omega) = 1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76592c97-e17e-447e-9cad-cd08094ce684",
   "metadata": {},
   "source": [
    "## Adding and Multiply Probability\n",
    "\n",
    "Two events which cannot occur together, or two propositions which cannot be true simultaenously, are called **mutually exclusive** or **disjoint**.\n",
    "\n",
    "For mutually exclusive events, probabilities add up.\n",
    "\n",
    "$Pr(A \\cup B) = Pr(A) + Pr(B)$ - A and B mutually exclusive\n",
    "\n",
    "For **independent events** / propositions, probabilities can be multiplied.\n",
    "\n",
    "$Pr(A \\land B) = Pr(A) * Pr(B)$ - if A and B are independent\n",
    "\n",
    "Often we deal with **compound events**. For example throwing two dice can be seen as a single compound event, formed of two elementary events. Compound events are often comprised of elementary events which are independent, which means we can multiply the probability of the compound event by multiplying the probabilities of the constituent elementary events. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a6a4c-6a55-40b5-b966-d008b38d21b7",
   "metadata": {},
   "source": [
    "# Conditional statements vs categorical statements\n",
    "\n",
    "A **categorical statement** may be \"The probability India will beat Australia is 40%\".\n",
    "\n",
    "But, suppose a key player is injured. We make the following **conditional statement** : \"Given X is injured, the probability India will beat Australia is 30%\".\n",
    "\n",
    "Conditional probability of event A, given B is said to be Pr(A/B).\n",
    "\n",
    "e.g. \n",
    "\n",
    "> Pr(second card dealt is ace) = 4/52 * 3/51 + 48/52 * 4/51  \n",
    "> Pr(second card dealt is ace/first card is not an ace) = 4/51  \n",
    "\n",
    "$$Pr(A/B) = \\frac{Pr(A \\land B)}{Pr(B)} \\text{  : given Pr(B) > 0}$$ \n",
    "\n",
    "A slightly different way to think of this is : What is the probably of A having happened, given B has happened? For example, what is the probably of the roll of dice being 6, given it is even? The answer is of course 1/3.\n",
    "\n",
    "In the first case, A and B are **overlapping events**. In the second case, A is a subset of B i.e. $Pr(A \\land B) = Pr(A)$. But the formula is the same in either case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493e5dd5-200e-4593-bcd5-f31c8fafa25b",
   "metadata": {},
   "source": [
    "## Basic Rules of Probability for Finite Sample Spaces\n",
    "\n",
    "Assumptions :  \n",
    "* The rules are for finite groups of propositions (or events)\n",
    "* If A and B are propositions (or events) so are $A \\land B, A \\lor B, \\neg A$\n",
    "* Elementary deductive logic is taken for granted\n",
    "* If A and B are *logically equivalent*, then Pr(A) = Pr(B) \n",
    "\n",
    "**normality** : $$0 \\leq Pr(A) \\leq 1$$  \n",
    "\n",
    "**certainty** : Given $\\Omega$ is a certain proposition or a sure event, $$Pr(\\Omega) = 1$$, \n",
    "\n",
    "**additivity** : Provided A and B are mutually exclusive, $$Pr(A \\lor B) = Pr(A) + Pr(B)$$.\n",
    "\n",
    "**overlap**.  : Provided A and B overlap i.e. are not mutually exclusive, $$Pr(A \\lor B) = Pr(A) + Pr(B) - Pr(A \\land B)$$ \n",
    "\n",
    "Proof:\n",
    "\n",
    "$A \\lor B \\equiv (A \\land B) \\lor (A \\land \\neg B) \\lor (\\neg A \\land B)$  \n",
    "\n",
    "$Pr(A \\lor B) = Pr(A \\land B) + Pr(A \\land \\neg B) + Pr(\\neg A \\land B)$  - since RHS are mutually exclusive  \n",
    "\n",
    "$Pr(A \\lor B) = Pr(A \\land B) + Pr(A \\land \\neg B) + Pr(A \\land B) + Pr(\\neg A \\land B) - Pr(A \\land B)$   \n",
    "\n",
    "$Pr(A \\lor B) = Pr(A) + Pr(B) - Pr(A \\land B)$   \n",
    "\n",
    "\n",
    "**conditional probability**: $$Pr(A/B) = Pr(A \\land B)/Pr(B) \\text{ if Pr(B) > 0}$$  \n",
    "\n",
    "**multiplication** : $$Pr(A \\land B) = Pr(A/B)Pr(B) \\text{ if Pr(B) > 0}$$  \n",
    "\n",
    "**total probability** : $$Pr(A) = Pr(B)Pr(A/B) + Pr(\\neg B)Pr(A/ \\neg B) \\text{ if 0 < Pr(B) < 1}$$  \n",
    "\n",
    "**logical consequence** : If B implies A i.e. $Pr(B) = Pr(A \\land B)$ : $$Pr(A) = Pr(A \\land B) + Pr(A \\land \\neg B) = Pr(B) + Pr(A \\land \\neg B)$$\n",
    "\n",
    "**statistical independence** : If 0 < Pr(A) and 0 < Pr(B), then, $$Pr(A/B) = Pr(A)$$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7918113b-ff2a-46e7-8571-486abaa1c3b9",
   "metadata": {},
   "source": [
    "## Rules for Conditional Probability\n",
    "\n",
    "The classic problem in logic is determining the truth of the conclusion given the truth of the premises. In the face of uncertainty, it amounts to asking the probability of the conclusion c being true, assuming the premises p are true.\n",
    "\n",
    "\n",
    "**normality** : $$0 \\leq Pr(A/E) \\leq 1$$  \n",
    "\n",
    "**certainty** : Given $\\Omega$ is a certain proposition or a sure event, $$Pr(\\Omega/E) = 1$$ \n",
    "\n",
    "> because $Pr(\\Omega/E) = Pr(\\Omega \\land E)/P(E)$.   \n",
    "> If $\\Omega \\land E \\equiv E$, since $\\Omega$ is always true.  \n",
    "> Hence $Pr(\\Omega/E) = Pr(E)/P(E) = 1$.  \n",
    "\n",
    "**additivity** : Provided A and B are mutually exclusive, $$Pr(A \\lor B/E) = Pr(A/E) + Pr(B/E)$$.\n",
    "\n",
    "Proof:  \n",
    "> $Pr(A \\lor B/E) = Pr((A \\lor B) \\land E)/Pr(E)$  \n",
    "> $(A \\lor B) \\land E = (A \\land E) \\lor (B \\land E)$ - and these are mutually exclusive as well.  \n",
    "> Hence, $Pr(A \\lor B) = Pr(A \\land E) + Pr(B \\land E)$   \n",
    "> Hence, $Pr(A \\lor B/E) = (Pr(A \\land E) + Pr(B \\land E))/Pr(E) = Pr(A \\land E)/Pr(E) + Pr(B \\land E)/Pr(E)$  \n",
    "> Hence, $Pr(A \\lor B/E) = Pr (A/E) + Pr(B/E)$  \n",
    "\n",
    "\n",
    "**overlap**.  : Provided A and B overlap i.e. are not mutually exclusive, $$Pr(A \\lor B) = Pr(A) + Pr(B) - Pr(A \\land B)$$ \n",
    "\n",
    "Proof:\n",
    "\n",
    "> $A \\lor B \\equiv (A \\land B) \\lor (A \\land \\neg B) \\lor (\\neg A \\land B)$  \n",
    "> $Pr(A \\lor B) = Pr(A \\land B) + Pr(A \\land \\neg B) + Pr(\\neg A \\land B)$  - since RHS are mutually exclusive  \n",
    "> $Pr(A \\lor B) = Pr(A \\land B) + Pr(A \\land \\neg B) + Pr(A \\land B) + Pr(\\neg A \\land B) - Pr(A \\land B)$   \n",
    "> $Pr(A \\lor B) = Pr(A) + Pr(B) - Pr(A \\land B)$   \n",
    "\n",
    "**conditional probability**: if Pr(E) > 0 and Pr(B/E) > 0 : $$Pr(A/(B \\land E)) = Pr((A \\land B)/E)/Pr(B/E)$$  \n",
    "\n",
    "Proof:\n",
    "\n",
    "> $Pr(A/(B \\land E)) = Pr(A \\land B \\land E)/Pr(B \\land E)$  \n",
    "> But, $Pr(A \\land B \\land E) = Pr(A \\land B/E)Pr(E)$  \n",
    "> And, $Pr(B \\land E) = Pr(B/E)Pr(E)$  \n",
    "> So, the formular follows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f049558-1833-4120-9000-043eed9267c7",
   "metadata": {},
   "source": [
    "## Independence in terms of Conditional Probability\n",
    "\n",
    "Revisiting the definition of independence - of the lack of \"memory\" etc. We can make this definition precise as follows :\n",
    "\n",
    "Pr(A/B) = Pr(A)\n",
    "\n",
    "We will show that :\n",
    "\n",
    "$$Pr(B/A) = Pr(B)$$  \n",
    "\n",
    "$$Pr(B \\land A) = Pr(A)Pr(B)$$  \n",
    "\n",
    "We can define A,B,C are **statistically independent** if A,B,C are pair-wise independent and\n",
    "$$Pr(A \\land B \\land C) = Pr(A)Pr(B)Pr(C)$$  \n",
    "\n",
    "Note that pair-wise independence **does not** imply statistical independence - the additional condition above is required.\n",
    "\n",
    "Proof:\n",
    "\n",
    "> Given $Pr(A/B) = Pr(B \\land A)/Pr(B) = Pr(A)$  \n",
    "> It follows that, as required : $Pr(B \\land A) = Pr(A)Pr(B)$  \n",
    "> It also follows : $Pr(B) = Pr(B \\land A)/Pr(A) = Pr(B/A)$ ... i.e. B is independent A as well  \n",
    "\n",
    "\n",
    "Example of Pairwise Independence not implying mutual independence:\n",
    "\n",
    "> Toss a coin twice. Consider the events : A. Both tosses are same B. First is heads C. Second is heads.  \n",
    "> You can see $Pr(A) = Pr(B) = Pr(C) = 1/2$  \n",
    "> $Pr(A \\land B) = Pr(B \\land C) = Pr (A \\land C) = 1/4$  \n",
    "> We can easily see that Pr(A/B) = A, and so on i.e. A,B,C are pairwise indepedent  \n",
    "> But $Pr(A \\land B \\land C) = 1/4$ since 2 heads is the common element.  \n",
    "> While Pr(A)Pr(B)Pr(C) = 1/8. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a3819-0fc8-4d06-8cc2-4abfbc8a656c",
   "metadata": {},
   "source": [
    "## Bayes' Rule\n",
    "\n",
    "Consider two steps : We toss a fair coin and then, based on that we pick a ball from one of two urns. We find the ball is red (event E).\n",
    "\n",
    "Let H1 = probability that urn 1 was chosen, and H2 = probability that urn 2 was chosen. \n",
    "\n",
    "Assume Pr(E/H1) = 0.4 and Pr(E/H2) = 0.2\n",
    "\n",
    "The question is : Which is the most likely urn which was selected based on the coin toss, given we know the ball is red.\n",
    "\n",
    "Then we need to calculate Pr(H1/E) and Pr(H2/E).\n",
    "\n",
    "We can recast this in terms of thinking of experiments, hypothesis and evidence.\n",
    "\n",
    "The experiment is what we have carried out - tossing a coin, picking a ball.\n",
    "\n",
    "The evidence is that we got a red ball (event E occured).\n",
    "\n",
    "The hypoothesis are H1 and H2. In this case H1 and H2 are mutually exclusive and exhaustive. Then :\n",
    "\n",
    "$$Pr(H1/E) = \\frac{Pr(H1)Pr(E/H1)}{Pr(H1)Pr(E/H1) + Pr(H2)Pr(E/H2)}$$\n",
    "\n",
    "Given evidence E, and a hypotheses $H_1, H_2, H_3,..H_k$ are mutually exclusive, cumulatively exhaustive (MECE), and for each of the hypotheses $Pr(H_i) > 0$\n",
    "\n",
    "$$Pr(H_j/E) = \\frac{Pr(H_j)Pr(E/H_j)}{\\sum_i Pr(H_i)Pr(E/H_i)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeebe75-095a-4722-9723-a6514f34599d",
   "metadata": {},
   "source": [
    "## Utility - Expected Value\n",
    "\n",
    "Logic analyzes reasons and arguments. The arguments may lead us to take a decision to alter oour **beliefs**, or undertake some **action**.\n",
    "\n",
    "Decisions depend on two things :\n",
    "\n",
    "* What we believe\n",
    "* What we want / value\n",
    "\n",
    "Sometimes we can represent our beliefs with probabilities, and the **utility** of what we value with an objective measure.\n",
    "\n",
    "We take **actions** based on reasons, which have **consequences** which may occur with some probability and ultimately these consequences have some value, or **utility** to us.\n",
    "\n",
    "Consider action - A with two consequences $C_1, C_2$. We have a utility function U which assigns some value to a consequence - this can be a dollar value, a quantity of happiness etc.\n",
    "\n",
    "$$Exp(A) = Pr(C_1/A)U(C_1) + Pr(C_2/A)U(C_2)$$\n",
    "\n",
    "Generalizing :\n",
    "\n",
    "$$Exp(A) = \\sum Pr(C_i/A)U(C_i)$$\n",
    "\n",
    "One way to identify the right decision / action is to identify the action which maximizes utility. Utilities could be a monetary value, or some other measure, measured in general units called **utiles**.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d96e5-0a50-4bea-ac49-082cfde77717",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6d57e-d5e2-4c14-9f06-76a538e8b501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
