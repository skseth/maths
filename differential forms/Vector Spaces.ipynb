{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Spaces over a field\n",
    "\n",
    "Bases, Dimension\n",
    "\n",
    "Solution to systems of linear equations\n",
    "\n",
    "Linear Operators\n",
    "\n",
    "Matrices representinng linear operators\n",
    "\n",
    "Determinants and the concept of volume\n",
    "\n",
    "Inner Product \n",
    "\n",
    "https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $R^n$ or $C^n$\n",
    "\n",
    "R and C are fields - let us refer to them as F, and to $R^n$ and $C^n$ as $F^n$.\n",
    "\n",
    "$F^n$ is the set of all lists of length n of elements of F. The jth element of the an element of $F^n$ is said to be its jth **coordinate**. \n",
    "\n",
    "0 is an element of $F^n$ with all coordinates as 0.\n",
    "\n",
    "**addition** : $(x_1,x_2,..,x_n) + (y_1,y_2,..,y_n) = (x_1 + y_1,x_2 + y_2,..,x_n + y_n)$ - addition is **commutative**, as the underlying fields are commutative. 0 is the **additive identity**. If x is an element of $F^n$, its **additive inverse** is -x.\n",
    "\n",
    "\n",
    "**scalar multiplication** \n",
    "\n",
    "$F^{S}$ - set of functions from S to F. $R^{[0,1]}$\n",
    "\n",
    "$F^N$  set of all sequences on F.\n",
    "\n",
    "(f + g)(x) - sum of functions\n",
    "$(\\lambda f)(x) = \\lambda\\ f(x)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System of linear equations\n",
    "\n",
    "https://www.math.ust.hk/~mabfchen/Math111/Week1-3.pdf\n",
    "\n",
    "\n",
    "Consider the following system of linear equations. These will be used in examples through the rest of the worksheet :\n",
    "\n",
    "Equation set (A)\n",
    "2x + y - 2z = -6\n",
    "3x - 3y - z = 5\n",
    "x - 2y + 3z = 6\n",
    "\n",
    "Ans (1,-1,1)\n",
    "\n",
    "Vector form :\n",
    "\n",
    "We can see the equation of a line in $R^3$ through point p0, and in direction of vector v can be written (using t as a parameter) :\n",
    "\n",
    "p = p0 + tv \n",
    "\n",
    "A plane containing 2 vectors v1 and v2 and a point p0 can be written (with 2 parameters t1 and t2)\n",
    "\n",
    "p = p0 + t1 v1 + t2 v2\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Spaces Over Fields\n",
    "\n",
    "In the following, u,v,w are elements of V, and a,b, 1 are elements of F. 0 refers to both the 0 of the vector space V and the field F.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Definition : Vector Space\n",
    "</div>\n",
    "\n",
    "A **vector space V over a field F** is a set V together with a binary operation called addition (+) on V, and a scalar multiplication . : FxV -> V, which have the following properties :\n",
    "\n",
    "Addition is commutative, associative, and has an identity element (0) and every element has an additive inverse. Since this means vector spaces form a group under addition, it follows that the identity and inverse are unique. \n",
    "    \n",
    "u + v = v + u\n",
    "u + (v + w) = (u + v) + w\n",
    "u + 0 = 0\n",
    "For every v, there is a unique additive inverse -v, such that v + (-v) = 0\n",
    "\n",
    "Scalar multiplication is distributive over vector and scalar addition (i.e. addition over the field), and the multiplicative identity (1) of the field is also an identity for scalar multiplication\n",
    "\n",
    "1v = v\n",
    "(a + b)v = av + bv\n",
    "a(v + u) = av + au\n",
    "\n",
    "Note : Whether we write av or va makes no difference - but this does not mean multiplication of a scalar and a vector is commutative since this just represents an ordered pair (a,v). On the other hand (ab)v = (ba)v because the underlying field F is commutative, by definition of a field. We have more general modules defined over rings, where the underlying \"ring\" is not commutative. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "A vector space over R is called an <b>R-vector space</b>\n",
    "    \n",
    "A vector space over C is called an <b>C-vector space</b>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "If 0 is the additive inverse in F, then 0u = 0. \n",
    "\n",
    "Note that here 0 refers to both the 0 of field F and 0 in the vector space V.\n",
    "</div>\n",
    "\n",
    "Proof: 0u = (0 + 0)u = 0u + 0u. Hence 0u + (-0u) = 0 = (0u + 0u) + (-0u) = 0u + (0u + (-0u)) = 0u + 0 = 0u.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "If -1 is the inverse of the multiplicative identity (1) in F, then the (-1)v = -v. \n",
    "</div>\n",
    "\n",
    "Proof: (-1)v + 1v = (-1 + 1)v = 0v = 0\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subspaces\n",
    "\n",
    "Vector spaces become interesting because of you can \n",
    "\n",
    "A **subspace** of a vector space is a subset which is also a vector space under the same operations as the containing vector space. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "For a subset of a vector space to be a <b>subspace</b>, it has to contain 0 and be closed under addition, and scalar multiplication.\n",
    "</div>\n",
    "\n",
    "Proof: Obiously if u,v are in the subspace, and a is any value in the scalar field, then the closure property means u + v, au, and a(u + v) and -u (inverses) are in the subspace. Commutativity, associativity, identity and additive inverse follow immediately. Similarly, scalar multiplication properties also follow so the subspace is a vector space.\n",
    "\n",
    "Subspaces are just what we expect. For $R^3$, subspaces are the origin {0}, lines through the origin, planes through the origin, and the whole space. \n",
    "\n",
    "An **intersection of two subspaces** is a subspace => e.g. intersection of two planes through origin may be a line through the origin. \n",
    "\n",
    "Proof: If W is an intersection of a collection of subspaces, if w,w' are in the intersection so is w + w', and aw. And of course 0 is in the intersection. Obviously the union of a subspace and another subspace which is a subset is a subspace. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Theorem : A union of two subspaces in general is not a subspace, but is a subspace if one is a subset of the other. In fact, a union of a finite collection of subspaces is only a subspace if one contains all the others (assuming the underlying field is infinite).\n",
    "</div>\n",
    "\n",
    "Proof: Assume $V = \\cup_{i=1}^{n} U_i$, where $U_i$ are subspaces, and V itself is a subspace. \n",
    "\n",
    "Now pick one vector v from $U_1$ and u from $v - U_1$, such that u is non-zero, and not in $U_1$. If no such u exists than $U_1$ is a superset of $v - U_1$ and the theorem is proved. \n",
    "\n",
    "If we have such a u, then v + ku, k != 0 cannot be in $U_1$ => v + ku must be in $v - U_1$. And because the field is infinite, k can take an infinite number of non-zero values. The number of subspaces is finite. By the pigeonhole principle, at least one $U_j$, j != 1, must contain $v + k_1u$ and $v + k_2u$, where $k_1$ and $k_2$ != 0. But we can then, by taking inverses, and linear combinations, immediately see that both y and x must belong to $U_j$. Thus $U_1 \\subset U_j$. Hence, $V = \\cup_{i=2}^{n} U_i$. Continuing in this fashion we will always reach one set which contains all the others - either in the middle, or the last one remaining.\n",
    "\n",
    "The proof works because we know that there is more than one k. But if F is a field of 2 elements, then the only value k can take is 1. Which means that we can have x + y in $U_j$, but this does not mean that $U_j$ has y, and hence x also. So the proof fails for such a field (called a field of characteristic 2 because 1 + 1 = 0).\n",
    "\n",
    "An example of a subspace in 3 dimensions is the plane : **2x + y - 2z = 0**\n",
    "\n",
    "\n",
    "\n",
    "We can define sums of subsets as follows : \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Suppose $U_1, U_2, ..., U_m$ are subsets (not necessarily subspaces) of V. The <b>sum of subsets</b> denoted is \n",
    "$$U_1 + U_2 + \\text{ ... } + U_m = {u_1 + u_2 + ... + u_m : u_i \\in U_i, i = 1..m}$$. Since vector addition is commutative, obviously so is the sum. Associativity also follows equally well.\n",
    "\n",
    "A <b>sum of subspaces</b> is just a sum as defined above, where every subset is a subspace. \n",
    "</div>\n",
    "\n",
    "A sum of subspaces includes as a subset every subspace involved in the direct sum (this can be proved easily).\n",
    "\n",
    "Note that the subspace {0} is an additive identity for the sum of subspaces, since 0 is a member of every subspace. But U is a subspace then U = U + U, since every subspace is closed under addition, U + V = U does not imply V = 0.\n",
    "\n",
    "For a subspace V, is there as (inverse) subspace W such that V + W = {0}? Since a subspace is contained in the sum, this is only possible if V and W are both {0}.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "The <b>sum of subspaces</b> is the smallest containing subspace    \n",
    "</div>\n",
    "\n",
    "Proof: Given $U_1, U_2, ... , U_m$ are subspaces of V. Let U be their sum. Then 0 obviously belongs to U. And for any 2 members $u = u_1, u_2, ..., u_m$ and $v = v_1, v_2, ..., v_m$, the vectors $u_1 + v_1, u_2 + v_2$ etc belong to U_1, U_2 and so on, which means u + v also belongs to U. And $au = au_1 + au_2 + ... + au_m$ is also in U, because $au_1, au_2, .. au_m$ belong to $U_1, U_2,...,U_m$ respectively. Thus U is a subspace. Obviously it is the smallest subspace containing the sum, because it is the sum.\n",
    "\n",
    "If we sum two lines through the origin we get a plane. If we sum a plane and a line through the origin, as can be seen, you will get the whole $R^3$ space (assuming the line is not in the plane). But 2 planes through the origin will also give us the $R^3$ space. This fact leads us to the definition of a direct sum :\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "A sum of subspaces is a direct sum if every vector in the direct sum is formed by a unique sum of vectors drawn from each of the subspaces forming the sum. We denote this by $U_1 \\oplus U_2 \\oplus \\text{ ... } \\oplus U_m$\n",
    "</div>\n",
    "\n",
    "Two planes through the origin do not form a direct sum, but a line and plane not containing the line do.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Theorem : A sum of subspaces is a direct sum iff 0 can be written in only one way as a combination of vectors from each of the subspaces (in fact this way has to be the sum of the 0's from each space itself). <br>\n",
    "</div>\n",
    "\n",
    "Proof of Theorem: (<=) Obviously, if a sum is a direct sum holds, 0 can be written in only one way.\n",
    "(=>) Assume $U_1, U_2, .., U_m$ are subspaces. Assume 0 can be written in only one way as a sum of vectors from these subspaces. We have to show the sum of the subspaces U is a direct sum i.e. every vector in U can be written in only one way. First, 0 = 0 + 0 + ... + 0 - so any other way of getting 0 must reduce to the same. Let u = (u1 + u2 + ... + um) = (v1 + v2 + ... + vm). Then 0 = (u1 - v1) + (u2 - v2) + .. + (um - vm) => u1 = v1, u2 = v2, ... um = vm.\n",
    "\n",
    "    \n",
    "<div class=\"alert alert-info\">\n",
    "Theorem : The sum of any two subspaces can be a direct sum iff their intersection is the zero element. This result does not extend to more than two subspaces, even if their pairwise intersection is the zero element.\n",
    "\n",
    "$$U_1 + U_2 = U_1 \\oplus U_2 \\text{ iff } U_1 \\cap U_2 = {0}$$    \n",
    "</div>\n",
    "\n",
    "Proof : (=>) Assume U and V are subspaces whose sum is a direct sum. If U and V intersected at two points 0 and v, say. Then 0 = 0 + 0 = v + (-v) violating the condition for a direct sum. -v exists in both subspaces because they are subspaces.\n",
    "(<=) Assume U and V are subspaces intersecting only at 0. If 0 = u + v, where u and v are not zero => u = (-v). But then u must be in V also, since V is a subspace, violating the hypothesis.\n",
    "\n",
    "The restriction to two sets can be surprising. If three subspaces exist which only intersect, pairwise, at 0, then they may still not form a direct sum. Informally this is because it may be possible to form vectors of one subspace by adding vectors of the other two - this breaks the condition for a direct sum. Consider :\n",
    "\n",
    "$U_1 = (x,y,0), U_2 = (0,0,z) \\text{ and } U_3 = (0,y,y)$ - $U_1$ is the xy plane, and $U_2$ is the z-axis. But $U_3$ is a slanted line in the yz plane. Thus (0,1,1) = (0,1,0) + (0,0,1), meaning that any sum written with (0,1,1) can also be written by replacing it with (0,1,0) and (0,0,1). \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Theorem : If $U_1, U_2,..,U_m$ are subspaces of a vector space V, then their sum is a direct sum iff $U_i \\cap \\sum_{j \\neq i} U_j = {0}$ for i = 1,2..m.  Also if $U = U_1 \\oplus U_2 \\oplus .. \\oplus U_m$, them dim U = dim U_1 + dim U_2 + .. + dim U_m, and the union of basis vectors of each space together form a basis of U.\n",
    "</div>\n",
    "\n",
    "Proof: (=>) Assume $U = \\sum_i U_i = \\oplus_i U_i$. Let $W = \\sum_{j \\neq 1} U_j$. Assume that $U_1 \\cap W$ contains a non-zero vector x, where $x = u_2 + u_3.. u_m$ for some $u_i \\in U_i$, i = 2..m. Then U_1 will also contain -x. So we could form 0 = 0 + 0 .. m times, as well as 0 = (-x) + x = (-x) + u_2 + u_3 + .. u_m, violating the definition for a direct sum. Thus $U_1 \\cap W = {0}$. We can proceed in the same way for $U_2,..U_m$.  \n",
    "(<=) Assume $U_i \\cap \\sum_{j \\neq i} U_j = {0}$ for i = 1,2..m. We need to show $U = \\sum_i U_i$ is a direct sum. Proceeding as earlier, we write $0 = u_1 + u_2 + .. + u_m = u_1 + w_1$. But this implies $-w_1 = u_1$ belongs to $U_1 \\cap \\sum_{j \\neq 1} U_j$ => $u_1 = 0$. We can show similarly $u_2,..,u_m = 0$ which imples the sum is a direct sum.  \n",
    "(basis vectors) Given $U = U_1 \\oplus U_2 ... \\oplus U_m$, Let B be the collection of all basis vectors of $U_1,..U_m$, with B_ij being the jth basis vector of $U_i$, j = 1..dim $U_i$. Now, $0 = \\sum_{i = 1}^{m} u_i = \\sum_{i=1}^{m}\\sum_{j=1}^{dim U_i} a_{ij} B_{ij}$. Since each of $u_1, u_2,..,u_m = 0$, this implies each of $a_{ij} = 0$. Thus the vectors B are linearly independent. Secondly for any u in U, $u = \\sum_{i=1}^{m}\\sum_{j=1}^{dim U_i} a_{ij} B_{ij}$, thus $U \\subset span(B)$. On the other hand, any linear combination of vectors in B can be written as a sum of vectors in $U_1, U_2,..,U_m$, so $span(B) \\subset U$ i.e. U = span(B). So B is a basis of U.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Corollary : A sum is a direct sum iff the dimensions add up.\n",
    "</div>\n",
    "\n",
    "Proof: (=>) We have already seen that if a sum is a direct sum, the basis vectors of the individual subspaces form basis vectors of the direct sum. So obviously the dimensions add up.  \n",
    "(<=) Assume the dimensions add up i.e. $U = U_1 \\oplus U_2 \\oplus .. \\oplus U_m$ and dim U = dim $U_1$ + dim $U_2$ + .. + dim $U_m$. Let B be the collection of basis vectors of U. Every vector in U can be expressed as a linear combination of B, so $U \\in span(B)$. Now 0 can also be expressed as a linear combination of B. If B are not linearly independent, this would imply dim U < sum of dimensions of subspaces, contradicting the assumption. But this implies that $U_i \\cap \\sum_{j \\neq i}U_j = {0}$, because otherwise, if there was a non-zero vector in the intersection, we could show vectors in B are linearly dependent. It follows that the sum is a direct sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Products and Quotients\n",
    "\n",
    "Besides subspaces, we can create new vector spaces by multiplying vector spaces (e.g. to form $R^2$, $R^3$ etc). \n",
    "\n",
    "Suppose $V_1, V_2,..,V_m$ are vector spaces over F. We define their **product** :\n",
    "\n",
    "$$\\prod_{i=1}^{m}V_i = {(v_1,v_2,..,v_m): v_i \\in V_i, i = 1..m}$$\n",
    "\n",
    "We define addition and scalar multiplication in the usual way, and with this the product forms a vector space.\n",
    "\n",
    "**Theorem: A product can be expressed as a direct sum of the vector spaces. This is often referred to as the external direct sum.**  \n",
    "Proof: We equate $V_1$ with $V_1$ x {0} x {0} .., $V_2$ with 0 x $V_2$ x {0} .... and so on. We can see that for $V_i \\cap \\sum_{j \\neq i} V_j = {0}$, hence the sum is a direct sum.\n",
    "\n",
    "Products give rise to new spaces. Can we form quotients? In some ways, we have seen subspaces which is a way of dividing a space, but this was limits to subsets which contain the origin. But what about subspaces not going through the origin? For this, we have **affine subset**  \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "An <b>affine subset</b> of V is a subset of V of the form v + U for some v in V and some subspace U of V. Here \n",
    "    \n",
    "$$v + U = \\{v + u : u \\in U \\}$$.\n",
    "    \n",
    "The affine subset (v + U) is said to be parallel to the subspace U, and to any other affine subset parallel to U.\n",
    "</div>\n",
    "\n",
    "In other words, an affine subset is formed by translation of a subspace (hence does not contain the origin). An affine subset is not a subspace, obviously.\n",
    "\n",
    "An example of an affine subset in 3 dimensions is the plane : **2x + y - 2z = -6** - this is a translation of the earlier plane.\n",
    "\n",
    "Actually all affine subsets parallel to each other fill the entire space. We define :\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Let U be a subspace of a vector space V. The <b>quotient space V/U</b> is the set of all affine subsets of V parallel to U.\n",
    "\n",
    "$$V/U = \\{v + U : v \\in U\\}$$\n",
    "</div>\n",
    "\n",
    "For example, if U is subspace **2x + y - 2z = 0**, then the quotient space V/U would be the set of all planes 2x + y - 2z = k where k is any real number. Before we prove the quotient space is a vector space, we need to clarify the definition of an affine subset - the primary issue is that given distinct vectors v and w, it is possible for v + U and w + U to specify the same affine subset, because it is possible for v = u + w, where u is a vector in U.\n",
    "\n",
    "We have the following theorem :\n",
    "\n",
    "<div>\n",
    "Theorem: Given V = v + U and W = w + U as affine subsets, V = W iff $v - w \\in U$. Otherwise V and U are disjoint.\n",
    "</div>\n",
    "Proof: (<=) Suppose $v-w \\in U$. Then $v + u = w + (v - w) + u \\in w + U$, and $w + u = v - (v - w) + u \\in v + U$. Thus V = W. (=>) Suppose V and W intersect at any point. Then there exist $u_1$ and $u_2$ in U, such that $v + u_1 = w + u_2$ => $v - w = u_2 - u_1$ => $(v - w) \\in U$. But this implies V and W are the same. Thus two affine subsets of a subspace U are either the same, or disjoint - they cannot just intersect at one point.\n",
    "\n",
    "Thus, we see that V and W as shown above are same only when $(v - w) \\in U$. \n",
    "\n",
    "We now define addition and scalar multiplication of quotient spaces :\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Theorem: Addition and multiplication of quotient spaces is defined on V/U by :\n",
    "    \n",
    "$$(v + U) + (w + U) = (v + w) + U$$ \n",
    "    \n",
    "$$\\lambda(v + U) = \\lambda\\ v + U  $$\n",
    "    \n",
    "With these definition, V/U forms a vector space.\n",
    "</div>\n",
    "\n",
    "Proof: (Addition is well defined) : We have to show that given an affine subset V = v + U = v' + U, and another subset W = w + U = w' + U, v + w + U = v' + w' + U. We know from previous theorem, that (v - v') and (w - w') are in U. Thus (v - v') + (w - w') are in U => (v + w) - (v' + w') are in U => v + w + U = v' + w' + U.  \n",
    "(Scalar multiplication is well defined) : We have to show $av + U = av' + U$. Since v - v' is in U, av - av' is also in U, which implies that av + U = av' + U.  \n",
    "(V/U is a vector space): Addition is commutative (v + w + U = w + v + U), associative. The additive identity is 0 + U = U. If we take the subspace -v + U, then (v + U) + (-v + U) = 0 + U => -v + U is the inverse of v + U. Other definitions follow in a straightforward way.\n",
    "\n",
    "<div class=\"alert alert-info\">    \n",
    "A is an affine subset of V, iff given v and w in A, $av + (1-a)w$ is also in A.\n",
    "    \n",
    "If $v_1, v_2,..v_m$ are vectors in V, such that \n",
    "$$A = \\{a_1v_1 + .. + a_mv_m: a_1 + a_2 + .. + a_m = 1 \\}$$\n",
    "then A is an affine subset of V\n",
    "</div>\n",
    "\n",
    "Proof: \n",
    "(a) (=>) Given A is an affine subset, containing v and w, say. Then A = v + U = w + U, for some subspace U. And (v - w) is in U i.e. a(v-w) is in U. Given, this w + a(v - w) is in A => av + (1 - a)w is in A.  \n",
    "(<=) Given w in A, we will show U = A - w = {x - w: x in A} is a subspace of V. \n",
    "We know ax + (1 - a)w is in A => a(x - w) = ax + (1-a)w - w is in U. Thus U is closed under scalar multiplication. \n",
    "What about addition? If x,y are in a, x - w is in U, and also y - w. Choosing a = 1/2, we get 1/2(x + y) is in A, hence 1/2(x + y) - w is in U. Thus (x + y) - 2w = (x - w) + (y - w) is also in U, since U is closed under scalar multiplication. We thus see that U is a subspace, and A = a + U is an affine subset.\n",
    "\n",
    "(b) Let a_1 + a_2 + .. + a_m = 1. Note that this implies that $v_1$ is in A. Then $a_1 = 1 - \\sum_{i=2}^m a_i$. for every $v \\in A$, $v = v_1 + \\sum_{i=2}^{m}a_i(v_i - v_1)$. If we set U = span($v_2-v_1,v_3-v_1,..v_m-v_1$), then U is a subspace of V, and $\\sum_{i=2}^{m}a_i(v_i - v_1) \\in U$. Thus for every v in A, $v = v_1 + u$, where u is in U. Thus $A \\subset v_1 + U$. And similarly, if u is in U, we can verify that $v_1 + u$ is in A, because it meets the condition $a_1 + a_2 + .. + a_m = 1$. Thus $A = v_1 + U$ is an affine subset of V.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Maps\n",
    "\n",
    "V, W represent vector spaces over field F, which can be either R or C.\n",
    "\n",
    "A **linear map** from V to W is a function T:V -> W, which is additive and homogenous. L(V,W) is the set of all linear maps from V to W. A linear map from a vector space to itself is called a **linear operator**, and the corresponding space of functions is called L(V). A **linear functional** is a linear map from a vector space to the underlying field F, and the corresponding space is called the **dual space** of V, V'.\n",
    "\n",
    "We can prove that a linear map is completely defined by its value on the basis. \n",
    "\n",
    "**Theorem: Linear maps are defined by their values on a basis.** Suppose $v_1, v_2,..,v_n$ are a basis of V. Then there is a unique linear map T from V to W, such that $Tv_j = w_j$, where $w_1,w_2,..,w_n$ are a given set of vectors in W. (Note that the w's are not a basis in W - and they may not even be unique - we will see later that the only duplicated value is 0).\n",
    "\n",
    "Proof: (Existence) Define $T(c_1v_1 + c_2v_2 + .. + c_nv_n) = c_1w_1 + c_2w_2 + .. + c_nw_n$. We can see that this function meets the condition $Tv_i = w_i$. And meets the homogeneity and additivity conditions. Since every v can be written a linear combination of the $v_i$'s, T is well defined.\n",
    "\n",
    "(Uniqueness) Given a function such that $Tv_i = w_i$ for i = 1..n. It follows that for any $v = c_1v_1 + c_2v_2 + .. + c_nv_n$, $Tv = c_1Tv_1 + c_2Tv_2 + .. + c_nTv_n = c_1w_1 + c_2w_2 + .. + c_nw_n$ i.e. the definition we have given above is unique.\n",
    "\n",
    "**Theorem: Given V and W are finite-dimensional vector spaces, L(V,W) is a vector space with dimension dim V * dim W, with the following operations defined : Given S,T in L(V,W) and a in F, (S + T)(u) = Su + Tu. And (aT)(u) = aTu. The additive identity is the zero map such that 0(u) = 0.**\n",
    "\n",
    "Proof: (vector space): Let S and T be in L(V,W). Then (S + T)(u + v) = S(u+v) + T(u+v) = Su + Sv + Tu + Tv = (S + T)(u) + (S+T)(v). (S+T)(au) = S(au) + T(au) = aS(u) + aT(u) = a(S+T)(u) - thus (S+T) is a linear map, and L(V,W) is closed under addition. We can see from the proof that addition is commutative (just switch around the factors), associative and that scalar multiplication is distributive over addition. (S + 0)(u) = Su + 0u = Su. So 0 is the additive identity. We can define (-T)v = -Tv for all v. Then -T is the additive inverse of T. etc. Hence L(V,W) is a vector space.  \n",
    "(dimension) : Let $v_1,v_2,..,v_n$ be a basis of V, and $w_1,w_2,..w_m$ be a basis of W. We define a function $F_{ij}$ such that $F_{ij}(v_k) = w_i$ if k = j, else $F_{ij} = 0$. We will show this is a basis for L(V,W). Let T be a linear map from V to W such that $Tv_j = w'_j = \\sum_{i=1}^{m}a_{ij}w_i$. Thus $T = \\sum_{j=1}^{n}\\sum_{i=1}^{m} a_{ij}F_{ij}$. Thus $span(F_{ij}) = L(V,W)$. For independence, we have to show that for the 0 function, all the $a_{ij}$ are zero. But for the zero function,  $\\sum_{i=1}^{m}a_{ij}w_i = 0$, for all j. Then $a_{ij} = 0$ for all i,j, because the $w_i$ are a basis for W. Hence, we have $a_{ij} = 0$ when T = 0, meaning that $F_{ij}$ are linearly independent and form a basis. Hence dim L(V,W) = dim V * dim W.  \n",
    "(alt proof using isomorphism) : Let B be a bsis for V. Then $L(V,W) \\cong W^{B} \\cong W^{|B|}$. Thus dim L(V,W) = |B| dim W = dim V * dim W.\n",
    "\n",
    "\n",
    "**Product of linear maps**. The product of two linear maps is their composition : ST = S o T. This product is associative, respects the identity of domain and range, and is distributive over addition, and vice verse.\n",
    "\n",
    "Proof: Associativity follows from the fact that function composition is associative. If T: V -> W, then $TI_V(w) = T(u)$ and $I_WT(u) = I_W(Tu) = Tu$. Thus identity is respected. $(S_1 + S_2)T(u) = S_1(T(u)) + S_2(T(u)) = S_1Tu + S_2Tu$ and $S(T_1 + T_2)(u) = S(T_1u + T_2u) = ST_1u + ST_2u$\n",
    "\n",
    "\n",
    "**Null Space** The null space of a linear map T: V -> W is the subset of elements v of V such that Tv = 0. The null space is a subspace (it follows that T0 = 0 for a linear map).\n",
    "\n",
    "Proof: Let $u,v \\in null T$. The T(u + v) = Tu + Tv = 0 + 0 = 0. And T(au) = aT(u) = a.0 = 0. And of course, T0 = T(0 + 0) = T0 + T0 => T0 = 0. Thus null T is a subspace.\n",
    "\n",
    "**Range of a linear map** The range of a linear map T (called range T) is a subspace.  \n",
    "Proof: Let T: V -> W be a linear map. Given u,v in V, Tu + Tv = T(u + v) and T(au) = aT(u) => range T is closed under the vector space operations. And T0 = 0. So range T is a subspace.\n",
    "\n",
    "Actually, since null T is a subspace, we can see that we have a subspace W, such that $V = null\\ T \\oplus U$. Every vector v in V can be expressed as $v = z + u, Tv = Tu$ where z is in null T and u is in U. Also, if u,v belong to U, then $Tu = Tv => T(u - v) = 0 => u - v = 0 => u = v$. Thus if we define T':U -> W such that T'u = Tu, then T' is injective i.e. one-one. Thus T breaks V cleanly into two parts - one injective part, and one which maps to 0. We can see the effect of this in the following theorem :\n",
    "\n",
    "\n",
    "For finite dimensional linear maps, there is a clear relationship between null T and range T.\n",
    "\n",
    "**Theorem: Fundamental Theorem of Linear Maps** Suppose V is finite dimensional, and $T \\in L(V,W)$. Then range T is finite-dimensional, and \n",
    "\n",
    "$$dim\\ V = dim\\ null\\ T + dim\\ range\\ T$$\n",
    "\n",
    "Proof: We will first work with T' and U, where $V = null T \\oplus U$ and T': U -> W such that T'u = Tu. We show dim U = dim range T. Let $u_1,u_2,..u_m$ be a basis of U, and $u = a_1u_1 + a_2u_2 + .. a_mu_m$ be in U. Then $Tu = a_1Tu_1 + a_2Tu_2 + .. + a_mTu_m$. We claim $B = Tu_1, Tu_2,..Tu_m$ form a basis for range T'. Obviously span B = range T'. Also, if $a_1Tu_1 + a_2Tu_2 + .. + a_mTu_m = 0$, then $T(a_1u_1 + a_2u_2 + .. + a_mu_m) = 0$ which implies that $a_1, a_2,..a_m = 0$, since T is injective as shown above. Hence we see that B is a basis for range T'. Thus dim range T' = dim U. But range T' = range T, because every vector v in V can be expressed as $v = z + u$, where z is in null T and u is in U. And T(z + u) = T(z) + T(u) = T(u) = T'(u). Thus we see that dim range T = dim U. And finally, we have dim V = dim null T + dim U = dim null T + dim range T.\n",
    "\n",
    "**Two finite dimensional vector spaces are isomorphic if there is a linear map between them which is both injective and surjective (and hence is invertible). This is only possible when they have the same dimension - in fact any two finite dimensional vector spaces over the same field, with the same dimension are isomorphic. The inverse map is also a linear map.\n",
    "Given T:V ->W, T can be injective only when dim V <= dim W, and can be surjective only when dim V >= dim W.**\n",
    "\n",
    "Proof: Let T:V -> W be both injective. Then dim null T = 0 => dim V = dim range T. If T is surjective, then range T = W, and dim V = dim W. A function has an inverse only when it is both injective and surjective, so clearly if a linear map between V and W has an inverse, dim V = dim W. But is $T^{-1}$ a linear map? If T(v) = w, and T(v') = w', then T(v + v') = w + w'. It follows that $T^{-1}(w + w') = T(w) + T(w')$, and similarly we can show the same for scalar multiplication. So T has an inverse and it is a linear map. To see that any two vector spaces with the same dimension are isomorphic, consider a map T: V->W, which maps basis vectors from V to W. Such a map is injective, since range T = W, null T = 0. And hence the inverse map exists and is linear. Obviously, the other identities follow from the fundamental theorem. \n",
    "\n",
    "\n",
    "**For a linear operator T on L(V), the following are equivalent : T is invertible, T is injective and T is surjective**  \n",
    "Proof: Obviously T is invertible => T is injective and surjective. If T is injective, dim V = range T, so T is surjective. And if T is surjective, then range T = V, hence dim null T = 0 => T is injective and hence T is invertible.\n",
    "\n",
    "**If T: U -> V and S:V ->W are linear maps with U.V,W being finite-dimensional and dim V = dim W, then ST is invertible iff S and T are invertible. The inverse is\n",
    "$$(ST)^{-1} = T^{-1}S^{-1}$$**  \n",
    "\n",
    "Note : dim U is always equal to dim W in either hypothesis. If V is larger than U / W, then it is possible for ST to be invertible, but T will be injective but not surjective, S will be surjective, but not injective. \n",
    "\n",
    "Similarly even if U = V = W is the same space, we have to be careful if V is infinite dimensional. For example, Let T(z1,z2,..) = (0,z1,z2,..) and S(z1,z2,..) = (z2,z3,..). Then ST = I, but TS != I (you lose z1). \n",
    "\n",
    "Proof: (=>) Since there exists an invertible map between U and W, dim U = dim W, and hence dim U = dim V. ST is invertible. So there exists R:W -> U such that $R(ST) = I_U$ and $(ST)R = I_W$. Let u be such that Tu = 0. But $u = I_uu = RSTu = RS0 = 0$. Thus null T = {0} => T is injective. And hence T is invertible (since dim U = dim V, T is also surjective). Thus $RS = T^{-1}$ => $TRS = I_v$. With this we can see that S is injective, and hence invertible.\n",
    "\n",
    "(<=). Since S and T are invertible, dim U = dim V = dim W. Assume S and T are invertible. $STT^{-1}S^{-1} = I_W$, $T^{-1}S^{-1}ST = I_U$. This is in fact true of all functions, not just linear maps. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Every subspace U of V is part of a direct sum equal to V.\n",
    "$$\\exists W, s.t. V = U \\oplus W$$. Obviously dim W = dim V - dim U\n",
    "    \n",
    "If $w_1,w_2,..w_n$ are basis vectors of W iff $w_1 + U, w_2 + U,..w_n + U$ are basis vectors of $V/U$. This implies W and V/U are isomorphic and dim V/U = dim V - dim U.\n",
    "\n",
    "</div>    \n",
    "Proof: (subspace is part of direct sum): If $u_1,..u_m$ are basis vectors of U, extend this to a basis of V by adding $w_1,..,w_n$ and let W = span($w_1,..,w_n$). Then $V = U + W$, because we can express any vector v of V as a linear combination of the basis vectors, and hence write it as $v = u + w$ where u is in U and w is in W. And $U \\cap W = \\{0\\}$, because if any $v = u + w = 0$, then each of u and w must be zero, because the basis vectors are linearly independent. It follows that dim V = dim U + dim W.\n",
    "\n",
    "(Basis vectors of Quotient and W): (=>) Let $u_1,..u_m$ be basis vectors of U. We know any v can be expressed as $v = u + w$. Hence v + U = (u + w) + U = w + U. And we know every w is a linear combination of the basis vectors of W, thus if $w_1, w_2,..,w_n$ are basis vectors of W, then $w_1 + U$, $w_2 + U$.. are basis vectors of V/U. This also implies that they have the same dimension and are isomorphic.\n",
    "(<=) Assume $v_1 + U, v_2 + U, v_n + U$ are basis vectors of v + U. Hence $v + U = a_1v_1 + .. + a_nv_n + U$ for all v in V.  \n",
    "Let $v + U = 0$ (this is basically saying v + U = U), then $a_1,a_2,..,a_n = 0$ i.e $v = 0$. But then $v + u = u'$ => $u = u'$, where u and u' are in U. If $u = b_1u_1 + .. + b_mu_m$, then $a_1v_1 + .. + a_nv_n + b_1u_1 + .. + b_mu_m = b_1u_1 + .. + b_mu_m$ => $a_1v_1 + .. + a_nv_n + (b_1 - b_1)u_1 + .. + (b_m - b_m)u_m = 0$. All the coefficients are zero. Thus $v_1,..,v_n,u_1,..,u_m$ are linearly independent and form a basis for V (because as we know already V/U is isomorphic to W, dim V/U + dim U = dim V). Let $v = u + w$, where u is a vector in V and w is a vector in W. Then $v = b_1u_1 + .. + b_mu_m + c_1w_1 + .. + c_nw_n = b_1u_1 + .. + b_mu_m + c_1w_1 + a_1v_1 + .. + a_nv_n$ => $w = c_1w_1 + .. + c_nw_n = a_1v_1 + .. + a_nv_n$. Thus $v_1,..v_n$ are linearly independent and span W, because we can take u = 0. Thus $v_1,..v_n$ form a basis for W.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Suppose U is a subspace of V. The <b>quotient map</b> $\\pi$ is a linear map from V to V/U such that :\n",
    "    \n",
    "$$\\pi(v) = v + U$$\n",
    "\n",
    "Theorem: \n",
    "(a) For every transformation T: V -> W, there is a linear transformation S:V/U -> W, such that T = S o $\\pi$ iff U is in null T.\n",
    "(b) Suppose U is a subspace of V. Define $\\tau: L(V/U,W) \\to L(V,W)$ by $\\tau(S) = S\\ o\\ \\pi$. Then $\\tau$ is an injective linear map, and range $\\tau$ = { T in L(V,W): Tu = 0 for all u in U}$.\n",
    "</div>\n",
    "\n",
    "Proof: \n",
    "(a) Assume $T = S \\o \\pi$. Then $\\pi(v) = 0 + U$ when v is in U, hence (S o $\\pi$)(v) = 0 => Tv = 0. Thus U must be in null T. Is S a linear transformation? S(av + U) = T(av) = aTv = aS(v+U). And S(v + w + U) = T(v + w) = Tv + Tw = S(v + U) + S(w + U). \n",
    "(b) $\\pi(u) = 0$ for all u in U. So (S o $\\pi$)(u) = 0 for all u in U. Since every T can be expressed as S o $\\pi$, we can see that { T in L(V,W): Tu = 0 for all u in U} $\\subset$ range $\\tau$. Equally, given any S':V/U -> W, we can construct T:V -> W as follows : Tv = 0 if v is in U, Tv = S(v + U) if v is not in U. It is easy to verify that $T = S\\ o\\ \\pi$, since $\\pi(v) = 0$ when v is in U, (S o $\\pi$)(v) = 0 = Tv in this case. Otherwise, $$\\pi$(v) = v + U$, and (S o $\\pi$)(v) = S(v + U) = Tv again. So we see that { T in L(V,W): Tu = 0 for all u in U} = range $\\tau$. Clearly $\\tau$ is linear : $\\tau$(R + S) = (R + S) o $\\pi$, from which the result follows. Similarly we can see for $\\tau$(aS) = (aS) o $\\pi$ = a(S o $\\pi$). Thus $\\tau$ is a linear map. Is it injective? Assume T = S o $\\pi$ = S' o $\\pi$. For any non-zero v, S(v + U) = Tv = S'(v + U). And S(0 + U)  = S'(0 + U) = 0 always. So S = S' i.e. $\\tau$ is injective. It follows that range $\\tau$ is a subspace with dim range $\\tau$ = dim L(V/U,W) = (dim(V) - dim U)dim W. \n",
    "\n",
    "<div class=\"alert alert-info\">    \n",
    "Given a linear map T: V -> W, dim V/null T = dim range T. \n",
    "\n",
    "We can define a linear map T' from V/null T to W, such that T'(v + null T) = Tv. Then, T' is injective, range T' = range T => V/null T is isomorphic to range T.\n",
    "</div>\n",
    "\n",
    "Proof: (Proof T' is well defined) : Assume u + null T = v + null T. Then $(u - v) \\in null T)$. Thus T(u - v) = 0 i.e. T(u) = T(v) => T'(v + null T) = T'(u + null T).   \n",
    "(Proof T' is a linear map) : T'(v + w + null T) = T(v + w) = T(v) + T(w) = T'(v + null T) + T'(w + null T). T'(au + null T) = aT(u) = aT'(u + null T). Other properties can be proved as well.  \n",
    "(Proof T' is injective) : T'(0 + null T) = T(0) = 0. If T(v + null T) = 0 => T(v) = 0 => v is in null T, and v + null T = 0 + null T. Thus null T' = {0} i.e. T' is injective.  \n",
    "(Proof range T' = range T) : T'(v + null T) = Tv => range T' subset of range T. On the other hand, 0 is in range T'. And for any non-zero Tv, v is not in null T => T'(v + null T) = Tv. Thus range T subset of range T' i.e. they are equal.  \n",
    "(Proof of isomorphism): It follows that T' is a bijection between V/null T and range T => V/null T and range T are isomorphic.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual Spaces\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Theorem: A dual space has the same dimension as the vector space :\n",
    "$$dim V' = V$$\n",
    "</div>\n",
    "\n",
    "Proof: Proof is immediate : dim V' = dim L(V,F) = dim V * dim F = dim V * 1 = dim V.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Theorem: A dual space has the same dimension as the vector space :\n",
    "$$dim V' = V$$\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Theorem: Given a basis $v_1,v_2,..,v_n$ of V, the dual space V' has a basis called the <b>dual basis</b> $f_1,f_2,..,f_n$ such that :\n",
    "\n",
    "$$f_i(v_j) = \\delta_{ij}$$, \n",
    "where $\\delta_{ij} = 1$ if i = j, and  $\\delta_{ij} = 0$ otherwise. $\\delta_{ij}$ is called <b>Kronecker's Delta</b>.\n",
    "</div>\n",
    "\n",
    "Proof: Define $f_i$ as above. Is it linear? Let $v = \\sum_{j=1}^{n} a_jv_j$ $u = \\sum_{j=1}^{n} b_jv_j$. Then $f_i(v) = a_i$, $f_i(u) = b_i$ and $f_i(cu + dv) = ca_i + db_i$. Thus $f_i$ is linear. Do the $f_i$'s form a basis? We just have to save $f_i$ are linearly independent.\n",
    "\n",
    "Let $0 = a_1f_1 + a_2f_2 + .. + a_nf_n$ (here 0 is the 0 linear functional). Then $0v_i = a_i = 0$ for i = 1..n. Thus $f_i$ are linearly independent, and since dim V' = n, they must form a basis. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Definition: If $T \\in L(V,W)$ then the <b>dual map</b> of T is a linear map $T' \\in L(W',V')$ defined by $T'(g) = g\\ o\\ T$ for $g \\in W'$.\n",
    "</div>\n",
    "\n",
    "Proof (T' is a linear map): T'(f + g) = (f + g) o T = f o T + g o T = T'f + T'g. T'(af) = (af) o T = a(f o T) = aT'(f).\n",
    "\n",
    "This is a complex definition, with much to unpack. First point is that since T is a map from V to W, and g is a linear functional from W to F, it follows that g o T maps V to F, and is a linear functional on V. \n",
    "\n",
    "One way to think of this, in category terms, is that if we replace every vector space by its dual, and every linear map by the reverse mapping of the dual spaces, this is the mapping you get (in category theory terms). The mapping between a category and its opposite is achieved by a contravariant function, one where if $S o T$ in the original category is replaced by $T' o S'$ in the target category. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "It is often a convention that if the original basis vectors are $e_1,e_2,..,e_n$, Basis vectors are referred to as : $e^1,e^2,..e^n$. If this is a dual basis, then $e^i(e_j) = \\delta^i_j$, where $\\delta^i_j$ is the Kronecker's Delta.\n",
    "    \n",
    "Also, vectors in the original space are written as $v = \\sum_{i} v^ie_i$, while those in the dual space are written as $\\sum_{i} v_ie^i$. The vectors of the dual space are called covectors (for covariant vectors).\n",
    "\n",
    "Another convention (called the Einstein Summation Convention) is that the summation sign can be assumed when the meaning is clear. Thus we can write $v = v_ie^i$, and it is understood this is a summation over i.\n",
    "\n",
    "Why are some indices in superscripts and others in subscripts? Superscripts indicate that the coefficients vary contraveriantly to a change in basis (an increase in lengths of the basis elements reduces the coefficients). This is true in the original vector space, as can be easily seen - if we take basis as $2e_1,e_2,..$, then the first coordinate of all vectors will halve.\n",
    "\n",
    "So will the coefficients of the corresponding dual basis : the new dual basis will be $\\frac{1}{2}e^1, e^2,..$, and the corresponding covector will therefore become (2v_1,v_2) - because the dual basis transforms contravariantly, the covectors transform covariantly again. Thus, as we can see from the definitions, superscripts denote coefficients which vary contravariantly, subscripts denote coefficients which vary covariantly. \n",
    "    \n",
    "\n",
    "Great link : https://math.stackexchange.com/questions/729994/covariant-vectors-and-dual-spaces   \n",
    "\n",
    "    \n",
    "https://math.stackexchange.com/questions/622589/in-categorical-terms-why-is-there-no-canonical-isomorphism-from-a-finite-dimens (TODO - understand this one day).\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Algebraic properties of dual maps\n",
    "\n",
    "For all S,T in L(V,W)\n",
    "(S + T)' = S' + T' \n",
    "(aT)' = aT'\n",
    "\n",
    "for T in L(U,V), S in L(V,W)\n",
    "(ST)' = T'S' \n",
    "</div>\n",
    "Proof: (of (ST)' = T'S'): (ST)'(g) = g o (ST) = (g o S) o T = T'(g o S) = T'(S'(g)) = T'S'(g). Basically S'(g) convert g to a linear functional over V. And T' then converts S'(g) to a linear functional over U.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "The <b>annihilator</b> $U^0$ of a subset U of V is the set of all linear functionals in V' which send U to 0.\n",
    "$$U^0 = \\{g \\in V': g(u) = 0\\ \\forall u \\in U \\}$$\n",
    "\n",
    "Theorem: $U^0$ is a subspace of V', and $dim\\ U + dim\\ U^0 = dim\\ V$. \n",
    "</div>\n",
    "\n",
    "Proof: Obviously 0 is in $U^0$. If f and g are in $U^0$, then (f + g)(u) = f(u) + g(u) = 0, and f(au) = af(u) = 0. So $U_0$ is closed under addition and scalar multiplication. \n",
    "\n",
    "Proof (of dimension) : Let $u_1,u_2,..,u_m$ be a basis of U, and extend this to a basis of V by adding $u_{m+1},..,u_{m+n}$.  Corresponding dual bases are defined as $u^i$. Then if g is in $U^0$, we can express $g = \\sum_{i=1}^{m+n} a_iu^i$. Since g(u) = 0 for all u in U, we must have g(u_i) = 0 for all  i = 1..m, we can also express $g = \\sum_{j=1}^{n} a_iu_{m+j}$ i.e. $u_{m+j}$ form a basis of $U^0$. Thus $dim\\ U^0 + dim\\ U = dim V$.\n",
    "\n",
    "(Fancier proof of dimension) : Let i in L(U,V) be the inclusion map from U to V (i(u) = u if u in U). Then i' is the dual map from V' to U'. such that if g is a linear functional in V', then i'(g) = g o i is a linear functional in U'. It follows that if (g o i)(u) = g(i(u)) = g(u) = 0 for all u in U, then g is in $U_0$. Hence null i' = $U^0$. Secondly i' is surjective i.e. range i' = U'. If h is a linear functional on U, then we can extend this to a linear functional f in V - thus i'(f) = f o i = h. Hence range i' = U', and dim range i' = dim U' = dim U.\n",
    "\n",
    "Based on this knowledge, we apply the fundamental theorem of linear maps to i' :\n",
    "\n",
    "dim V = dim null i' + dim range i' = dim $U^0$ + dim U.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Suppose V and W are finite dimensional,and T is in L(V,W). Then :\n",
    "    \n",
    "$null\\ T' = (range\\ T)^0$\n",
    " \n",
    "$dim\\ null\\ T' = dim\\ null\\ T + dim\\ W - dim\\ V = dim\\ W - dim\\ range\\ T$   \n",
    "    \n",
    "Corollary (Relation between T and T') :  \n",
    "$dim\\ range\\ T' = dim\\ range\\ T$  \n",
    "$range\\ T' = (null\\ T)^0$  \n",
    "T is injective iff T' is surjective. T is surjective iff T' is injective.\n",
    "</div>\n",
    "\n",
    "Proof: If g is in null T', then T'(g) = g o T = 0 => g(Tv) = 0 for all v. Thus g is in $(range\\ T)^0$. On the other hand, let h be in $(range\\ T)^0$. This implies that h(Tv) = 0 => T'(h) = 0. Thus h is in null T'.\n",
    "\n",
    "Secondly, $dim\\ (range\\ T)^0 + dim\\ (range\\ T) = dim\\ W$, from which the result follows.  \n",
    "(Corollary) (a) dim range T' = dim W - dim null T' = dim W - dim $(range\\ T)^0$ = dim range T.  \n",
    "(b) Let $g = T'(h)$, and z be in null T. g = (h o T)(z) = h(0) = 0, for some h: W' -> V'. Thus $g \\in (null\\ T)^0$. Thus $range\\ T' \\subset (null\\ T)^0$. But dim range T' = dim range T = dim $(null\\ T)^0$. Thus $range\\ T' = (null\\ T)^0$.  \n",
    "(c) If T is injective, null T = {0} and it follows that dim V = dim range T = dim range T' = dim V' i.e. T' is surjective. It is easy to go the other way : dim range T' = dim V => dim range T = dim V => T is injective. \n",
    "If T is surjective, then $(range T)^0$ is {0}, i.e. null T' = {0} => T' is injective. If T' is injective, null T' = $(range T)^0$ = {0} => dim range T = dim W and T is surjective.\n",
    "\n",
    "\n",
    "By Exercise 1, all the $\\varphi$'s are surjective. Consider the following process\n",
    "\n",
    "Step 1.\n",
    "\n",
    "Choose $v_1 \\in V$ such that $\\varphi_1(v_1) = 1$.\n",
    "\n",
    "Step j.\n",
    "\n",
    "If $j = n + 1$, stop the process. By the contrapositive of the statement in Theorem 2 of Chapter 3 notes, it follows that there is a vector $v_j \\in V$ such that $v_j \\in \\bigcap_{1 \\le k \\le n, k \\neq j}$ and $v_j \\notin \\operatorname{null} \\varphi_j$. Because both these subspaces are closed under scalar multiplication and because $\\varphi_j$ is surjective, we can assume without loss of generality that $\\varphi_j(v_j) = 1$.\n",
    "\n",
    "After step $n$ the process stops and we will have a list $v_1, \\dots, v_n$ such that $\\varphi_j(v_k) = 1$ if $j = k$ and $\\varphi_j(v_k) = 0$ if $j \\neq k$. We but need to prove that $v_1, \\dots, v_n$ is linearly independent, since it already has length $\\operatorname{dim} V$.\n",
    "\n",
    "Suppose there are $a_1, \\dots, a_n \\in \\mathbb{F}$ such that\n",
    "\n",
    "$$ a_1 v_1 + \\dots + a_n v_n = 0 $$\n",
    "\n",
    "Applying $\\varphi_j$ to both sides of the equation above gives $a_j = 0$, for each $j = 1, \\dots, n$. Hence $v_1, \\dots, v_n$ is linearly independent and, therefore, a basis of $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices\n",
    "\n",
    "## System of Linear Equations\n",
    "\n",
    "A system of linear equations are a set of m equations in n variables:\n",
    "\n",
    "$$\\sum_{i=1}^{m} a_{ij}x_j = b_i$$\n",
    "\n",
    "If all $b_i = 0$, the equations are said to be homogenous. Homogenous equations represent subspaces. inhomogenous equations represent affine subsets of the space (a fancy word for subsets which do not pass through the origin but are are parallel to a subspace).\n",
    "\n",
    "The other is to view them as a linear combination of column vectors : If we let $A_{.,j} = \\begin{bmatrix}a_{1j}\\\\a_{2j}\\\\..\\\\a_{mj}\\end{bmatrix}$, $b = \\begin{bmatrix}b_1\\\\b_2\\\\..\\\\b_m\\end{bmatrix}$ and $x = \\begin{bmatrix}x_1\\\\x_2\\\\..\\\\x_n\\end{bmatrix}$ then we can represent the set of equations as : \n",
    "\n",
    "$$x_1A_{.,1} + x_2A_{.,2} + .. + x_nA_{.,n} = b$$\n",
    "\n",
    "Or as an m x n matrix A : \n",
    "\n",
    "$$Ax = b$$\n",
    "\n",
    "Where $$A = \\begin{bmatrix}A_{.,1} & A_{.,2} & .. & A_{.,n}\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "If m = n, it is possible the equation has a unique solution. Suppose we have n equations in n variables of this type : Ax = 0, and another, with the same A, Ax = b (where b is a set of values)\n",
    "\n",
    "Then saying that the first (Ax = 0) has a unique solution is the same as saying (Ax = b) has a unique solution. It happens because the underlying linear map is invertible. To see this consider, remember every A represents a transformation T:\n",
    "\n",
    "$$T(x_1,x_2,..,x_n) = (\\sum_{i=1}^n a_{1,k}x_1, \\sum_{i=1}^n a_{2,k}x_2,..,\\sum_{i=1}^n a_{n,k}x_n)$$\n",
    "\n",
    "If Ax = 0 has only one solution (obviously the trivial solution 0), this means null T = {0}. This means T is injective, and since this is an n x n matrix, T is surjective as well. Thus every \"b\" has a solution i.e. there is a unique point $(x_1,x_2,..,x_n)$ which is translated to any given b.\n",
    "\n",
    "\n",
    "## Matrix of a Linear Map M(T)\n",
    "\n",
    "Given a set of basis vectors $v_1,v_2,..,v_m$ of V, and $w_1, w_2,..,w_m$ of W and a linear map T: V -> W, we know that each $v_j$ can be represented as $Tv_j = w = \\sum_{i=1}^{m}a_ijw_i$. Also this uniquely specifies a linear map. \n",
    "As above, if we declare $A_j$ as a column vector, then $Tv_j = A_{.,j}$, the column vector in the m-dimensional space W. The m x n matrix $A = \\begin{bmatrix}A_{.,1} & A_{.,2} & .. & A_{.,n}\\end{bmatrix}$ completely represents the transformation T. And if $x = x_1v_1 + x_2v_2 + .. + x_nv_n$, then we can see that $Tx = x_1Tv_1 + x_2Tv_2 + .. + x_nTv_n$ which is represented by the matrix $Ax = x_1A_{.,1} + x_1A_{.,2} + .. + x_nA_{.,n}$.  \n",
    "\n",
    "Thus, once we fix the bases, A completely represents the transformation T, and is called M(T) or the matrix of T (for the specified bases). Addition and scalar multiplication are defined exactly as expected, so M(S + T) = M(S) + M(T), and aM(T) = M(aT).\n",
    "\n",
    "For matrix multiplication, let T: V -> W be represented by an m x n matrix with entries $a_{jk}$, and S: W -> U be represented by a p x m matrix, with entries represented by $b_{ij}$, then ST: V -> U is represented by an p x n matrix, with each entry represented by :\n",
    "\n",
    "$$(ST)_{ik} = \\sum_{j=1}^{m} b_{ij}a_{jk}$$\n",
    "\n",
    "This results in the equation : $M(ST) = M(S)M(T)$\n",
    "\n",
    "**$F^{m,n}$ is the set of all m x n matrices**. We can see that this is a vector space. It is also fairly clear that the matrix has dimension m x n (basis vectors are the mn m x n matrices with only 1 non-zero entry equal to 1). But we can also see that each matrix represents one transformation T, $F^{m,n}$, we can see that this is isomorphic to L(V,W), which implies that they have the same dimension, mn.\n",
    "\n",
    "**Matrix of a vector** : We know that given basis vectors $v_1, v_2,..v_n$, we can represent any vector v as $v = c_1v_1 + c_2v_2 + .. + c_nv_n$. We can think of $M(v) = \\begin{bmatrix}c_1\\\\c_2\\\\..\\\\c_n\\end{bmatrix}$, with respect to this basis vector. And if T: V -> W is represented by a matrix M(T), then Tv is represented by the matrix M(T)M(v).\n",
    "\n",
    "## Transpose - Matrix of Dual Map M(T')\n",
    "\n",
    "The **transpose** of a matrix A is defined by exchanging the rows and columns of A.\n",
    "\n",
    "$$(A^t)_{j,i} = (A)_{i,j}$$\n",
    "\n",
    "The transpose of a sum is the sum of the transposes.\n",
    "\n",
    "$$((A + B)^t)_{j,i} = (A + B)_{i,j} = A_{i,j} + B_{i,j} = (A^t)_{j,i} + (B^t)_{j,i}$$\n",
    "\n",
    "Scalar multiplications works as expected as well.\n",
    "$$((aA)^t)_{j,i} = (aA)_{i,j} = a(A)_{i,j} = a(A^t)_{j,i}$$\n",
    "\n",
    "However, \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "$$(AC)^t = C^tA^t$$\n",
    "</div>\n",
    "\n",
    "Proof: Suppose A is an m x n matrix, and C is an n x p matrix, then AC is an m x p matrix, and AC^t is a p x m matrix.\n",
    "\n",
    "$((AC)^t)_{k,i} = (AC)_{i,k} = \\sum_{j=1}^{n} A_{i,j}C_{j,k} = \\sum_{j=1}^{n} (C^t)_{k,j}(A^t)_{j,i} = (C^tA^t)_{k,i}$ where i = 1..m, k=1..p. \n",
    "\n",
    "Like the dual map, transposing reverses the order of composition. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Assume we have $v_1,v_2,..v_n$ as basis vectors of V and $w_1,w_2,..w_n$ as basis vectors of W. And let dual basis vectors be corresponding defined, with superscripts. Given a linear map T: V -> W, we have a unique matrix M(T) representing T with the given basis vectors in V and W. Then, the dual map T' is defined by :\n",
    "    \n",
    "$$M(T') = (M(T))^t$$\n",
    "    \n",
    "</div>\n",
    "\n",
    "Proof: Let A = M(T) and C = M(T'). We have, by definition : $T'(w^j) = \\sum_{r = 1}^{n} C_{r,j}v^r = w^j\\ o\\ T$. Thus $(w^j\\ o\\ T)(v_k) = \\sum_{r = 1}^{n} C_{r,j}v^r(v_k) = C_{k,j}$. But, we also have  $(w^j\\ o\\ T)(v_k) = w^j(Tv_k) = w^j(\\sum_{r=1}^{m}A_{r,k}w_r) = A_{j,k}$.\n",
    "\n",
    "## Rank of a Matrix\n",
    "\n",
    "The **column rank** of a matrix is the number of linearly independent columns of the matrix. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Suppose T is in L(V,W) with V,W finite-dimensional. Then dim range T = column rank of M(T). The row and column ranks of the matrix are the same, and referred to as the rank of the matrix.\n",
    "</div>\n",
    "\n",
    "Proof: Let M(T) = A. Let $v_1,v_2,..v_n$ be basis vectors of V, and $w_1,w_2,..w_n$ be basis vectors of W. Given a matrix $A = \\begin{bmatrix}A_{.,1} & A_{.,2} & .. & A_{.,n}\\end{bmatrix}$ represents the columns of the matrix. $Tv_j = A_{.,j}w_i$. Let $w \\in span(Tv_1,Tv_2,..Tv_j)$ and M(w) = span(A_{1,j}, A_{2,j},..,A_{.,n}). We can see that the two spans are isomorphic, hence dim range T = column rank of matrix M(T).\n",
    "\n",
    "Now row rank of M(T) = column rank of M(T') = dim range T' = dim range T = column rank of M(T).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules Over Rings\n",
    "\n",
    "https://kurser.math.su.se/pluginfile.php/14420/mod_resource/content/1/yishao-zhou-DifferenceModulesVS.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about infinite vector spaces\n",
    "\n",
    "http://www.math.lsa.umich.edu/~kesmith/infinite.pdf\n",
    "\n",
    "Span(B) = ....\n",
    "\n",
    "Polynomials\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
